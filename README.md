![](https://img.shields.io/github/stars/pandao/editor.md.svg) ![](https://img.shields.io/github/forks/pandao/editor.md.svg) ![](https://img.shields.io/github/tag/pandao/editor.md.svg) ![](https://img.shields.io/github/release/pandao/editor.md.svg) ![](https://img.shields.io/github/issues/pandao/editor.md.svg) ![](https://img.shields.io/bower/v/editor.md.svg)


### Introduction
The project aims to detect and recognize faces. The final objective is to code the identified faces and export the data for application in other projects, such as searching social networks by facial recognition, searching in databases and applications in Marketing, how to qualify Leads to prospect possible customers who frequent your establishment.

The code is divided into four modules, making the system easier to read and understand. We ask that, before using these modules, you fully understand how each one communicates with the other.

### Modules
In this topic we will explain the basics of how all modules work to facilitate the use and reproduction of the code. [Opencv-python](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) and [Mediapipe](https://google.github.io/mediapipe/) are essential for understanding. We recommend that you have a base of usage for each extension.

#### [main.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/main.py)
We start by activating the Detector class of the [detector_module.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/detector_module.py) module. In this segmentation are most of the system properties, including whether you will proceed with the treatment and encoding of the exported images. Pay attention to the method:

<pre><code>with detection_system.mp_face_detector.FaceDetection(min_detection_confidence=0.2,
                                                     model_selection=1) as machine_face_detector:
</code></pre>

We recommend that you read the [Mediapipe Framework Concepts](https://google.github.io/mediapipe/framework_concepts/gpu.html) documentation to understand how it works and the need to reduce processing costs. You will better understand how the [detector_module.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/detector_module.py) module works. In the loop are the system specs and the video/camera frame refresh feature. Afterwards, the treatment of the exported images and the coding of the faces.

#### [detector_module.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/detector_module.py)
In order, we will continue to explain the construction of the `Detector()`. Pay attention to the __init__ function and its parameters. Some internal processing parameters such as `mp.solutions.face_detection` should have more of your attention.

| Function name  | Description                    |
| -------------  | ------------------------------ |
| `Detector()`   | Initial Parameters             |
| `stack_image()`| Generate a generic image stack |
| `Capture()`    | Process the image with the specified machine |
| `Bbox()`       | Extract and save the clippings of the original video with the bounding boxes |
| `Grid()`       | Generate an image grid simultaneously with the video |
| `VideoOutput()`| Display window containing processed videos |
| `RecordVideo()`| Records videos |
| `ReleaseCap()` | Release the camera/video used and destroy all windows generated by opencv |

In the static methods, we have the `stack_image()` function which is a generalization of ways of stacking images. It will serve to show the user the soft search being performed on the video/camera. In the `Capture()`, we have the machine parameter that will be used an argument of the class itself `self.mp_face_detection` and after executed in the [main.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/main.py) file in the with operator. In addition, we have the treatment of each frame, based on what is recommended by Mediapipe and the execution of the face search.

![Face Detection with Mediapipe](https://github.com/RP11-AI/face-recognition-3SN/blob/main/readme-data/output_video.gif?raw=true)

In `Bbox()` there is a method of showing the bounding boxes in detections to the user. Also, and more importantly to proceed with processing, extracting each bounding box into a png file. These files will be the basis for the development of other modules. The `Grid()` will generate the stacking of images for simultaneous viewing. `VideoOutput()` is the function that will generate a window to display the videos.

![Grid Video with face detections](https://github.com/RP11-AI/face-recognition-3SN/blob/main/readme-data/output_grid_video.gif?raw=true)

#### [faceAuth.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/faceAuth.py)
The module will scan the images generated by the previous module. The scan method is predestined in the *faces_bbox* folder. The class is extremely simple, but very useful. The detection pattern is more critical, to filter out unnecessary images and make [faceRecognition.py](https://github.com/RP11-AI/face-recognition-3SN/blob/main/py/faceRecognition.py) easier to work with.

|Function name          | Description                    |
| --------------------- | ------------------------------ |
| `SecondaryDetection()`| It reads the png files from the indicated directory and loads the images into RAM for encoding. Contains more rigorous face detection. |
| `FaceAuth()`          | Detect faces in png files in the directory indicated in the __init__ function. The purpose of the function is to filter out unnecessary images, making AI coding easier. Recommended to keep the default settings. |

![Log - Face Auth](https://github.com/RP11-AI/face-recognition-3SN/blob/main/readme-data/2022-07-19%20120423.png?raw=true)
